# [NYC_TAXI Data Pipeline ğŸš•](https://github.com/trannhatnguyen2/NYC_Taxi_Data_Pipeline)

# Table Of Contents

<!--ts-->
-  [Getting Started](#-getting-started)
- [How to Guide](#-how-to-guide)
- [References](#-references)
  <!--te-->


# ğŸ“ Repository Structure

```shell
.
    â”œâ”€â”€ airflow/                                    /* Airflow setup and DAGs */
    â”‚   â””â”€â”€ dags/
    â”‚       â”œâ”€â”€ elt_pipeline_dag.py                 /* Basic ELT */
    â”‚       â””â”€â”€ elt_pipeline_optimized_dag.py       /* Optimized ELT incl. batch to DW */
    â”œâ”€â”€ batch_processing/
    â”‚   â””â”€â”€ datalake_to_dw.py                       /* Original batch (datalake â†’ DW) */
    â”œâ”€â”€ batch_processing_optimized.py               /* Optimized batch entrypoint */
    â”œâ”€â”€ config/                                     /* Project configs */
    â”‚   â”œâ”€â”€ spark.yaml
    â”‚   â”œâ”€â”€ datalake.yaml
    â”‚   â””â”€â”€ dbt_profiles.yml
    â”œâ”€â”€ data/                                       /* Sample parquet datasets */
    â”‚   â”œâ”€â”€ 2021/
    â”‚   â”œâ”€â”€ 2022/
    â”‚   â”œâ”€â”€ 2023/
    â”‚   â””â”€â”€ 2024/
    â”œâ”€â”€ data_validation/
    â”‚   â”œâ”€â”€ gx/
    â”‚   â”‚   â”œâ”€â”€ checkpoints/
    â”‚   â”‚   â”œâ”€â”€ expectations/
    â”‚   â”‚   â””â”€â”€ great_expectations.yml
    â”‚   â””â”€â”€ transform.ipynb
    â”œâ”€â”€ debezium/
    â”‚   â”œâ”€â”€ configs/
    â”‚   â”‚   â””â”€â”€ taxi-nyc-cdc.json                   /* Debezium connector config */
    â”‚   â””â”€â”€ run.sh
    â”œâ”€â”€ jars/                                       /* Spark extra JARs (S3A, Kafka, JDBC) */
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ data/taxi_lookup.csv
    â”‚   â”œâ”€â”€ extract_load.py                         /* local â†’ MinIO raw */
    â”‚   â”œâ”€â”€ transform_data.py                       /* raw â†’ processed */
    â”‚   â””â”€â”€ convert_to_delta.py                     /* processed â†’ delta (optional) */
    â”œâ”€â”€ streaming_processing/
    â”‚   â”œâ”€â”€ read_parquet_streaming.py
    â”‚   â”œâ”€â”€ schema_config.json
    â”‚   â”œâ”€â”€ streaming_fixed.py
    â”‚   â”œâ”€â”€ streaming_to_minio_fixed.py             /* Kafka â†’ MinIO (parquet) */
    â”‚   â””â”€â”€ streaming_to_datalake.py
    â”œâ”€â”€ trino/
    â”‚   â”œâ”€â”€ catalog/datalake.properties
    â”‚   â””â”€â”€ etc/{config.properties,jvm.config,node.properties}
    â”œâ”€â”€ utils/
    â”‚   â”œâ”€â”€ create_schema.py
    â”‚   â”œâ”€â”€ create_table.py
    â”‚   â”œâ”€â”€ postgresql_client.py
    â”‚   â”œâ”€â”€ helpers.py
    â”‚   â”œâ”€â”€ minio_utils.py
    â”‚   â”œâ”€â”€ streaming_data_json.py
    â”‚   â”œâ”€â”€ streaming_data_db.py
    â”‚   â””â”€â”€ trinp_db.py
    â”œâ”€â”€ airflow-docker-compose.yaml
    â”œâ”€â”€ docker-compose.yaml
    â”œâ”€â”€ stream-docker-compose.yaml
    â”œâ”€â”€ Makefile
    â”œâ”€â”€ README.md
    â””â”€â”€ requirements.txt
```

# ğŸš€ Getting Started

1.  **Clone the repository**:

    ```bash
    git clone <your-fork-or-repo-url>
    cd NYC_Taxi_Data_Pipeline
    ```

2.  **Start all infrastructures**:

    ```bash
    make run_all
    ```

    This command will download the necessary Docker images, create containers, and start the services in detached mode.

3.  **Setup environment**:

    ```bash
    conda create -n bigdata python==3.9 -y
    conda activate bigdata
    pip install -r requirements.txt
    ```

4.  **Access the Services**:

    - Postgres is accessible on the default port `5432`.
    - Kafka Control Center: `http://localhost:9021`.
    - Debezium Connect REST API: `http://localhost:8083` and Debezium UI: `http://localhost:8085`.
    - MinIO Console: `http://localhost:9001`.
    - Airflow UI: `http://localhost:8080`.
    - Spark UI (when a Spark job is running): `http://localhost:4040` (or `4041+`).

5.  **Download Dataset**:
    You can download and use this dataset here: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page

6.  **Download JAR files for Spark**:

    ```bash
    mkdir jars
    cd jars
    curl -O https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar
    curl -O https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar
    curl -O https://repo1.maven.org/maven2/org/postgresql/postgresql/42.4.3/postgresql-42.4.3.jar
    curl -O https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.2.1/spark-sql-kafka-0-10_2.12-3.2.1.jar
    ```

# ğŸ” How to Guide

## I. Batch Processing

### ğŸ“‹ Manual Step-by-Step

1.  **Start Infrastructure**

    ```bash
    make batch_up
    # Or manually start containers as needed
    ```

2.  **Upload local parquet â†’ MinIO `raw`**

    ```bash
    python scripts/extract_load.py
    ```

3. **Transform `raw` â†’ `processed` (Parquet)**

    ```bash
    python scripts/transform_data.py
    ```

4. **(Optional) Convert `processed` â†’ `delta`**

    ```bash
    python scripts/convert_to_delta.py
    ```

5. **Create database schema and staging tables**

    ```bash
    python utils/create_schema.py
    python utils/create_table.py
    ```

6. **Execute optimized Spark batch (Datalake â†’ DW)**

    ```bash
    # Recommended
    python batch_processing_optimized.py

    # Or original version
    python batch_processing/datalake_to_dw.py
    ```

### ğŸ“Š Performance Notes

- Complete pipeline: ~7â€“15 minutes (sample dataset)
- Optimized processing: handles 40M+ records efficiently
- Memory usage: optimized for ~4GB
- Timeout handling: retry and progress logging

7. **Validate data (optional)**

    ```bash
    cd data_validation
    # Configure expectations in gx/ if needed
    # Open the notebook for a quick exploration
    # transform.ipynb
    ```

8. **DBT models (star schema)**

    ```bash
    cd nyc_taxi
    # Configure profiles using config/dbt_profiles.yml
    # dbt deps
    # dbt build
    ```

## II. Stream Processing

### ğŸš€ Quick Start (CDC â†’ Kafka â†’ Spark â†’ MinIO)

1) Start services

```bash
make stream_up   # Zookeeper, Kafka, Schema Registry, Debezium, Control Center
make batch_up    # PostgreSQL, MinIO, Hive Metastore, Trino
```

2) Prepare database schemas/tables

```bash
python utils/create_schema.py
python utils/create_table.py
```

3) Register Debezium connector (Postgres â†’ Kafka)

```bash
cd debezium
curl -X POST -H "Content-Type: application/json" \
  --data @configs/taxi-nyc-cdc.json \
  http://localhost:8083/connectors
curl -X GET http://localhost:8083/connectors/taxi-nyc-cdc/status
cd ..
```

4) Produce data changes (CDC)

```bash
python utils/streaming_data_db.py   # Ctrl+C to stop when enough
```

5) Run Spark streaming job (Kafka â†’ MinIO)

```bash
python streaming_processing/streaming_to_minio_fixed.py
```

Notes:
- Watch console logs; Spark UI appears at `http://localhost:4040` while running.
- Kafka Control Center: `http://localhost:9021` â†’ Topics â†’ `device.iot.taxi_nyc_time_series` â†’ Messages.
- MinIO Console: `http://localhost:9001` â†’ bucket `raw/stream/` for Parquet outputs.

### (Optional) Query streaming data in MinIO with Trino

```bash
docker exec -ti datalake-trino bash
trino
```

```sql
CREATE SCHEMA IF NOT EXISTS datalake.stream
WITH (location = 's3://raw/');

CREATE TABLE IF NOT EXISTS datalake.stream.nyc_taxi(
    VendorID                INT,
    tpep_pickup_datetime    TIMESTAMP,
    tpep_dropoff_datetime   TIMESTAMP,
    passenger_count         DOUBLE,
    trip_distance           DOUBLE,
    RatecodeID              DOUBLE,
    store_and_fwd_flag      VARCHAR,
    PULocationID            INT,
    DOLocationID            INT,
    payment_type            INT,
    fare_amount             DOUBLE,
    extra                   DOUBLE,
    mta_tax                 DOUBLE,
    tip_amount              DOUBLE,
    tolls_amount            DOUBLE,
    improvement_surcharge   DOUBLE,
    total_amount            DOUBLE,
    congestion_surcharge    DOUBLE,
    airport_fee             DOUBLE
) WITH (
    external_location = 's3://raw/stream',
    format = 'PARQUET'
);
```

## III. Airflow - Data Orchestration

```bash
cd airflow/
```

See the detailed setup and usage in the Airflow README:

- Airflow guide: [airflow/README.md](airflow/README.md)

---

# ğŸ“Œ References

[1] [NYC Taxi Trip Dataset](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

---