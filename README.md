# [NYC_TAXI Data Pipeline ğŸš•](https://github.com/trannhatnguyen2/NYC_Taxi_Data_Pipeline)

# Table Of Contents

<!--ts-->
-  [Getting Started](#-getting-started)
- [How to Guide](#-how-to-guide)
- [References](#-references)
  <!--te-->


# ğŸ“ Repository Structure

```shell
.
    â”œâ”€â”€ airflow/                                    /* airflow folder including dags,.. /*
    â”œâ”€â”€ batch_processing/
    â”‚   â””â”€â”€ datalake_to_dw.py                           /* ETL data from datalake to staging area /*
    â”œâ”€â”€ configs/                                    /* contain config files /*
    â”‚   â”œâ”€â”€ spark.yaml
    â”‚   â””â”€â”€ datalake.yaml
    â”œâ”€â”€ data/                                       /* contain dataset /*
    â”‚   â”œâ”€â”€ 2020/
    â”‚   â”œâ”€â”€ 2021/
    â”‚   â”œâ”€â”€ 2022/
    â”‚       â”œâ”€â”€ green_tripdata_2022-01.parquet
    â”‚       â”œâ”€â”€ green_tripdata_2022-02.parquet
    â”‚       â”œâ”€â”€ green_tripdata_2022-03.parquet
    â”‚       â”œâ”€â”€ ...
    â”‚       â”œâ”€â”€ yellow_tripdata_2022-01.parquet
    â”‚       â”œâ”€â”€ yellow_tripdata_2022-02.parquet
    â”‚       â”œâ”€â”€ yellow_tripdata_2022-03.parquet
    â”‚       â””â”€â”€ ...
    â”‚   â”œâ”€â”€ 2023/
    â”‚   â””â”€â”€ 2024/
    â”œâ”€â”€ data_validation/                            /* validate data before loading data warehouse /*
    â”‚   â”œâ”€â”€ gx/
    â”‚       â”œâ”€â”€ checkpoints/
    â”‚       â”œâ”€â”€ expectations/
    â”‚       â”œâ”€â”€ ...
    â”‚       â””â”€â”€ great_expections.yml
    â”‚   â”œâ”€â”€ full_flow.ipynb
    â”‚   â””â”€â”€ reload_and_validate.ipynb
    â”œâ”€â”€ dbt_nyc/                                    /* data transformation folder /*
    â”œâ”€â”€ debezium/                                   /* CDC folder /*
    â”‚    â”œâ”€â”€ configs/
    â”‚       â””â”€â”€  taxi-nyc-cdc-json                           /* file config to connect between database and kafka through debezium  /*
    â”‚    â””â”€â”€ run.sh                                     /* run create connector */
    â”œâ”€â”€ imgs/
    â”œâ”€â”€ jars/                                       /* JAR files for Spark version 3.5.1 */
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ data/
    â”‚       â””â”€â”€ taxi_lookup.csv                             /* CSV file to look up latitude and longitude */
    â”‚   â”œâ”€â”€ extract_load.py                             /* upload data from local to 'raw' bucket (MinIO) */
    â”‚   â”œâ”€â”€ transform_data.py                           /* transform data to 'processed' bucket (MinIO) */
    â”‚   â””â”€â”€ convert_to_delta.py                         /* convert data parquet file from 'processed' to 'delta' bucket (MinIO) */
    â”œâ”€â”€ streaming_processing/
    â”‚    â”œâ”€â”€ read_parquet_streaming.py
    â”‚    â”œâ”€â”€ schema_config.json
    â”‚    â””â”€â”€ streaming_to_datalake.py               /* read data stream in kafka topic and write to 'raw' bucket (Minio) */
    â”œâ”€â”€ trino/
    â”‚    â”œâ”€â”€ catalog/
    â”‚       â””â”€â”€  datalake.properties
    â”‚    â”œâ”€â”€ etc/
    â”‚       â”œâ”€â”€ config.properties
    â”‚       â”œâ”€â”€ jvm.config
    â”‚       â””â”€â”€ node.properties
    â”œâ”€â”€ utils/                                     /* functions /*
    â”‚    â”œâ”€â”€ create_schema.py
    â”‚    â”œâ”€â”€ create_table.py
    â”‚    â”œâ”€â”€ postgresql_client.py                       /* PostgreSQL Client: create connect, execute query, get columns in bucket /*
    â”‚    â”œâ”€â”€ helper.py
    â”‚    â”œâ”€â”€ minio_utils.py                             /* Minio Client: create connect, create bucket, list parquet files in bucket /*
    â”‚    â”œâ”€â”€ streaming_data_json.py                     /* stream data json format into kafka */
    â”‚    â”œâ”€â”€ streaming_data_db.py                       /* stream data into database */
    â”‚    â””â”€â”€ trino_db_scripts_generate.py
    â”œâ”€â”€ .env
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ airflow-docker-compose.yaml
    â”œâ”€â”€ docker-compose.yaml
    â”œâ”€â”€ Makefile
    â”œâ”€â”€ README.md
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ stream-docker-compose.yaml
```

# ğŸš€ Getting Started

1.  **Clone the repository**:

    ```bash
    git clone 
    ```

2.  **Start all infrastructures**:

    ```bash
    make run_all
    ```

    This command will download the necessary Docker images, create containers, and start the services in detached mode.

3.  **Setup environment**:

    ```bash
    conda create -n bigdata python==3.9
    y
    conda activate bigdata
    pip install -r requirements.txt
    ```

    Activate your conda environment and install required packages

4.  **Access the Services**:

    - Postgres is accessible on the default port `5432`.
    - Kafka Control Center is accessible at `http://localhost:9021`.
    - Debezium Connect REST API at `http://localhost:8083` and Debezium UI at `http://localhost:8085`.
    - MinIO is accessible at `http://localhost:9001`.
    - Airflow is accessible at `http://localhost:8080`.
    - Spark UI (when a Spark job is running) is at `http://localhost:4040` (or `4041+`).

5.  **Download Dataset**:
    You can download and use this dataset in here: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page

6.  **Download JAR files for Spark**:

    ```bash
    mkdir jars
    cd jars
    curl -O https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar
    curl -O https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar
    curl -O https://repo1.maven.org/maven2/org/postgresql/postgresql/42.4.3/postgresql-42.4.3.jar
    curl -O https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.2.1/spark-sql-kafka-0-10_2.12-3.2.1.jar
    ```

# ğŸ” How to Guide

## I. Batch Processing

### **Quick Start - Complete Batch Processing**

Run the complete batch processing pipeline with one command:

```bash
# Complete automated test (recommended)
python test_complete_batch.py
```

This will automatically execute all steps below and provide detailed progress tracking.

### ğŸ“‹ **Manual Step-by-Step Process**

1.  **Start Infrastructure**:

```bash
# Start all required services
make batch_up
# Or manually start containers:
# docker run -d --name test-postgres -p 5432:5432 -e POSTGRES_USER=k6 -e POSTGRES_PASSWORD=k6 -e POSTGRES_DB=k6 postgres:13
# docker run -d --name test-minio -p 9000:9000 -p 9001:9001 -e MINIO_ACCESS_KEY=minio_access_key -e MINIO_SECRET_KEY=minio_secret_key minio/minio server /data --console-address ":9001"
```

2.  **Upload data from local to `raw` bucket (MinIO)**:

```bash
python scripts/extract_load.py
```

3. **Transform data from `raw` to `processed` bucket (MinIO)**:

```bash
python scripts/transform_data.py
```

4. **(Optional) Convert data to Delta Lake format**:

```bash
python scripts/convert_to_delta.py
```

5. **Create database schema and staging tables**:

```bash
python utils/create_schema.py
python utils/create_table.py
```

6. **Execute optimized Spark processing (Datalake â†’ Data Warehouse)**:

```bash
# Use optimized version (recommended - faster and more reliable)
python batch_processing_optimized.py

# Or use original version
python batch_processing/datalake_to_dw.py
```

### âš¡ **Quick Testing Options**

```bash
# Quick test with limited data (2 files only - for development/testing)
python test_batch_quick.py

# Full test with progress tracking and error handling
python test_complete_batch.py
```

### ğŸ“Š **Performance Notes**

- **Complete pipeline**: ~7-15 minutes for full dataset
- **Optimized processing**: Handles 40M+ records efficiently  
- **Memory usage**: Optimized for 4GB RAM allocation
- **Timeout handling**: Automatic retry and progress tracking


6. **Validate data in Staging Area**

```bash
   cd data_validation
   great_expectations init
   Y
```

Then, run the file `full_flow.ipynb`
]

7. **Use DBT to transform the data and create a star schema in the data warehouse**

```bash
   cd dbt_nyc
```

See the DBT project guide here:

- DBT models and usage: [nyc_taxi/README.md](nyc_taxi/README.md)


## II. Stream Processing

### ğŸš€ Quick Start (CDC â†’ Kafka â†’ Spark â†’ MinIO)

1) Start services
```bash
make stream_up   # Zookeeper, Kafka, Schema Registry, Debezium, Control Center
make batch_up    # PostgreSQL, MinIO, Hive Metastore, Trino
```

2) Prepare database schemas/tables
```bash
python utils/create_schema.py
python utils/create_table.py
```

3) Register Debezium connector (Postgres â†’ Kafka)
```bash
cd debezium
curl -X POST -H "Content-Type: application/json" \
  --data @configs/taxi-nyc-cdc.json \
  http://localhost:8083/connectors
curl -X GET http://localhost:8083/connectors/taxi-nyc-cdc/status
cd ..
```

4) Produce data changes (CDC)
```bash
python utils/streaming_data_db.py   # Ctrl+C to stop when enough
```

5) Run Spark streaming job (Kafka â†’ MinIO)
```bash
python streaming_processing/streaming_to_minio_fixed.py
```
Notes:
- Watch console logs; Spark UI appears at `http://localhost:4040` while running.
- Kafka Control Center: `http://localhost:9021` â†’ Topics â†’ `device.iot.taxi_nyc_time_series` â†’ Messages.
- MinIO Console: `http://localhost:9001` â†’ bucket `raw/stream/` for Parquet outputs.

### (Optional) Read/query streaming data in MinIO with Trino

After putting your files to ` MinIO`, please execute `trino` container by the following command:

```bash
docker exec -ti datalake-trino bash
trino
```

After that, run the following command to register a new schema for our data:

```sql

    CREATE SCHEMA IF NOT EXISTS datalake.stream
    WITH (location = 's3://raw/');

    CREATE TABLE IF NOT EXISTS datalake.stream.nyc_taxi(
        VendorID                INT,
        tpep_pickup_datetime    TIMESTAMP,
        tpep_dropoff_datetime   TIMESTAMP,
        passenger_count         DOUBLE,
        trip_distance           DOUBLE,
        RatecodeID              DOUBLE,
        store_and_fwd_flag      VARCHAR,
        PULocationID            INT,
        DOLocationID            INT,
        payment_type            INT,
        fare_amount             DOUBLE,
        extra                   DOUBLE,
        mta_tax                 DOUBLE,
        tip_amount              DOUBLE,
        tolls_amount            DOUBLE,
        improvement_surcharge   DOUBLE,
        total_amount            DOUBLE,
        congestion_surcharge    DOUBLE,
        airport_fee             DOUBLE
    ) WITH (
        external_location = 's3://raw/stream',
        format = 'PARQUET'
    );

```

## III. Airflow - Data Orchestration

```bash
   cd airflow/
```

See the detailed setup and usage in the Airflow README:

- Airflow guide: [airflow/README.md](airflow/README.md)

---

# ğŸ“Œ References

[1] [NYC Taxi Trip Dataset](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

---

